version: "3"
services:
    airflow_scheduler:
        container_name: airflow_scheduler
        build: airflow/
        restart: always
        depends_on:
            - webserver
        volumes:
            - ./dag_admin/dags:/usr/local/airflow/dags
        environment:
            - LOAD_EX=n
            - FERNET_KEY=$FERNET_KEY
            - EXECUTOR=Celery
            - AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=False
            - AIRFLOW__SCHEDULER__DAG_DIR_LIST_INTERVAL=10
        command: scheduler

    airflow_webserver:
        container_name: airflow_webserver
        build: airflow/
        restart: always
        depends_on:
            - postgres
            - redis
        environment:
            - AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=False
            - AIRFLOW__SCHEDULER__DAG_DIR_LIST_INTERVAL=10
            - LOAD_EX=n
            - FERNET_KEY=$FERNET_KEY
            - EXECUTOR=Celery
            - "PYTHONPATH=$PYTHONPATH:/usr/local/airflow/dags"
        volumes:
            - ./dag_admin/dags:/usr/local/airflow/dags
        ports:
            - "8081:8080"
        command: webserver
        healthcheck:
            test: ["CMD-SHELL", "[ -f /usr/local/airflow/airflow-webserver.pid ]"]
            interval: 30s
            timeout: 30s
            retries: 3

    airflow_worker:
        container_name: airflow_worker
        build: airflow/
        restart: always
        depends_on:
            - scheduler
        volumes:
            - ./dag_admin/dags:/usr/local/airflow/dags
        environment:
            - AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=False
            - FERNET_KEY=$FERNET_KEY
            - EXECUTOR=Celery
            - "PYTHONPATH=$PYTHONPATH:/usr/local/airflow/dags"
            - AIRFLOW__SCHEDULER__DAG_DIR_LIST_INTERVAL=10
        command: worker

    redis:
        container_name: redis
        image: redis:5.0.5
        restart: always

    postgres:
        container_name: postgres
        image: postgres:9.6
        restart: always
        environment:
            - POSTGRES_USER=airflow
            - POSTGRES_PASSWORD=airflow
            - POSTGRES_DB=airflow
        logging:
            driver: none

    flower:
        container_name: airflow_flower
        image: puckel/docker-airflow:1.10.4
        restart: always
        depends_on:
            - redis
        environment:
            - EXECUTOR=Celery
        ports:
            - "5555:5555"
        command: flower
        logging:
            driver: none

    mongo:
        container_name: mongo
        image: mongo:3.4.23
        ports:
        - "27017:27017"
        logging:
            driver: none

    infrastructure: 
        container_name: infrastructure
        environment:
        - SERVICE_PORT=5000
        build: infrastructure/ 
        ports: 
        - "5000:5000" 
        volumes:
        - ~/.aws:/root/.aws

    job_scheduler: 
        container_name: job_scheduler
        environment:
        - SERVICE_PORT=5001
        - PYSPARK_PYTHON=python3
        build: job_scheduler/ 
        ports: 
        - "5001:5001"
        volumes:
        - /:/app/root
        - ~/.aws:/root/.aws
        links:
        - infrastructure

    dag_admin: 
        container_name: dag_admin
        build: dag_admin/ 
        ports: 
        - "5000:5000"
        volumes:
        - ./dag_admin/dags:/app/backend/dags

    governance:
        container_name: governance
        build: governance/
        ports:
        - "8080:8080"
        depends_on:
        - mongo
        logging:
            driver: none

    webserver: 
        container_name: webserver
        build: webserver/ 
        ports: 
        - "8080:8080"
